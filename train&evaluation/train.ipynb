{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade datasets peft huggingface_hub bitsandbytes accelerate transformers torch torchvision nltk && %pip install huggingface_hub==0.15.1 peft==0.5.0 && %pip install -i https://test.pypi.org/simple/ bitsandbytes && %pip install -U bitsandbytes transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x11da86550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"offline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses load_dataset from the datasets library to load the \"knkarthick/dialogsum\" dataset, which is divided into train, validation, and test splits. The dataset is printed out, showing the structure with the number of rows and features (id, dialogue, summary, topic) for each split.\n",
    "It then converts the train split into a Pandas DataFrame, prints the first 5 rows, and checks the shape of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanchen/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n",
      "{'id': ['train_0', 'train_1', 'train_2', 'train_3', 'train_4'], 'dialogue': [\"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", \"#Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\", \"#Person1#: Excuse me, did you see a set of keys?\\n#Person2#: What kind of keys?\\n#Person1#: Five keys and a small foot ornament.\\n#Person2#: What a shame! I didn't see them.\\n#Person1#: Well, can you help me look for it? That's my first time here.\\n#Person2#: Sure. It's my pleasure. I'd like to help you look for the missing keys.\\n#Person1#: It's very kind of you.\\n#Person2#: It's not a big deal.Hey, I found them.\\n#Person1#: Oh, thank God! I don't know how to thank you, guys.\\n#Person2#: You're welcome.\", \"#Person1#: Why didn't you tell me you had a girlfriend?\\n#Person2#: Sorry, I thought you knew.\\n#Person1#: But you should tell me you were in love with her.\\n#Person2#: Didn't I?\\n#Person1#: You know you didn't.\\n#Person2#: Well, I am telling you now.\\n#Person1#: Yes, but you might have told me before.\\n#Person2#: I didn't think you would be interested.\\n#Person1#: You can't be serious. How dare you not tell me you are going to marry her?\\n#Person2#: Sorry, I didn't think it mattered.\\n#Person1#: Oh, you men! You are all the same.\", \"#Person1#: Watsup, ladies! Y'll looking'fine tonight. May I have this dance?\\n#Person2#: He's cute! He looks like Tiger Woods! But, I can't dance. . .\\n#Person1#: It's all good. I'll show you all the right moves. My name's Malik.\\n#Person2#: Nice to meet you. I'm Wen, and this is Nikki.\\n#Person1#: How you feeling', vista? Mind if I take your friend'round the dance floor?\\n#Person2#: She doesn't mind if you don't mind getting your feet stepped on.\\n#Person1#: Right. Cool! Let's go!\"], 'summary': [\"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", 'Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.', \"#Person1#'s looking for a set of keys and asks for #Person2#'s help to find them.\", \"#Person1#'s angry because #Person2# didn't tell #Person1# that #Person2# had a girlfriend and would marry her.\", \"Malik invites Nikki to dance. Nikki agrees if Malik doesn't mind getting his feet stepped on.\"], 'topic': ['get a check-up', 'vaccines', 'find keys', 'have a girlfriend', 'dance']}\n",
      "        id                                           dialogue  \\\n",
      "0  train_0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
      "1  train_1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
      "2  train_2  #Person1#: Excuse me, did you see a set of key...   \n",
      "3  train_3  #Person1#: Why didn't you tell me you had a gi...   \n",
      "4  train_4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
      "\n",
      "                                             summary              topic  \n",
      "0  Mr. Smith's getting a check-up, and Doctor Haw...     get a check-up  \n",
      "1  Mrs Parker takes Ricky for his vaccines. Dr. P...           vaccines  \n",
      "2  #Person1#'s looking for a set of keys and asks...          find keys  \n",
      "3  #Person1#'s angry because #Person2# didn't tel...  have a girlfriend  \n",
      "4  Malik invites Nikki to dance. Nikki agrees if ...              dance  \n",
      "(12460, 4)\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(ds['train'][:5])\n",
    "df = ds['train'].to_pandas()\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 12,460 rows in the train split, 500 rows in the validation split, and 1,500 rows in the test split, all containing id, dialogue, summary, and topic features.\n",
    "The first 5 rows display a sample of dialogues, summaries, and topics, showing conversations on various subjects like health check-ups, vaccines, and relationships.\n",
    "The shape of the train dataset is (12460, 4), confirming it has 12,460 rows and 4 columns, representing dialogues, summaries, and topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first shuffles the train dataset using a random seed (42) and selects the first 50 samples from the shuffled data using the select method.\n",
    "Similarly, it shuffles the validation dataset and selects the first 20 samples.\n",
    "A new DatasetDict is created, containing the selected train and validation subsets, which is then printed to show the resulting data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and then select 50 samples\n",
    "train_subset = ds['train'].shuffle(seed=42).select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and then select 20 samples\n",
    "validation_subset = ds['validation'].shuffle(seed=42).select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "# Create DatasetDict with the selected samples\n",
    "data_subsets = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"validation\": validation_subset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train subset now contains 50 rows with the same features (id, dialogue, summary, topic), as the data was shuffled and sampled.\n",
    "The validation subset contains 20 rows with the same feature structure after applying the shuffle and sample operations.\n",
    "The printed DatasetDict confirms the new size of both subsets: 50 samples for train and 20 samples for validation, preserving the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code imports the AutoTokenizer from the transformers library and loads the facebook/bart-base model's tokenizer.\n",
    "It defines a preprocessing function that tokenizes the input dialogues and summaries, applying padding and truncation to ensure fixed-length sequences.\n",
    "The preprocessing function is applied to the train and validation subsets, creating tokenized datasets, followed by removing the original text columns (like id, dialogue, summary, topic) to keep only the necessary model inputs (input_ids, attention_mask, and labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Define preprocessing function\n",
    "#max_source_length = 1024\n",
    "#max_target_length = 176\n",
    "max_source_length = 512\n",
    "max_target_length = 88\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize inputs and labels with padding and truncation\n",
    "    inputs = examples[\"dialogue\"]\n",
    "    targets = examples[\"summary\"]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_source_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize labels with padding and truncation\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=max_target_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    # Update model inputs with labels\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to the subset of training data\n",
    "tokenized_dataset = data_subsets.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns for the model\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'dialogue', 'summary', 'topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset now has 50 rows, with features input_ids, attention_mask, and labels, representing the tokenized inputs and their corresponding labels.\n",
    "The validation dataset contains 20 rows with the same features (input_ids, attention_mask, and labels).\n",
    "The printed DatasetDict confirms that the datasets are now preprocessed and tokenized, with the original columns removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize=False, lora=True\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "#adapt a large pre-trained model (like facebook/bart-base) for \n",
    "# a specific task without having to fine-tune all of its parameters\n",
    "#LoRA helps make training more efficient by updating only a small \n",
    "#subset of parameters, rather than the entire model, which reduces \n",
    "# the computational resources needed for training.\n",
    "print(\"Quantize=False, lora=True\")\n",
    "# Define LoRA Config \n",
    "# Define configuration parameters in a dictionary\n",
    "lora_config_params = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": TaskType.SEQ_2_SEQ_LM\n",
    "}\n",
    "\n",
    "# Pass the dictionary to LoraConfig using the ** unpacking operator\n",
    "lora_config = LoraConfig(**lora_config_params)\n",
    "# base_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 1,179,648 || all params: 407,470,080 || trainable%: 0.2895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanchen/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# add LoRA adaptor\n",
    "base_model = get_peft_model(base_model, lora_config)\n",
    "base_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "training_args_params = {\n",
    "    \"output_dir\": \"fine-tuned-bart\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"weight_decay\": 0.005,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 2,\n",
    "    \"save_strategy\": \"epoch\",  # Save less frequently\n",
    "    \"save_total_limit\": 1,\n",
    "    \"report_to\": None,\n",
    "    \"run_name\": \"facebook-bart-base-finetuning\",\n",
    "    \"predict_with_generate\": True,\n",
    "    \"fp16\": True\n",
    "}\n",
    "\n",
    "# Pass the dictionary to Seq2SeqTrainingArguments using the ** unpacking operator\n",
    "training_args = Seq2SeqTrainingArguments(**training_args_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/nanchen/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Sat Apr 20 12:58:46 2024) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n",
      "[nltk_data] Downloading package punkt to /Users/nanchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nanchen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Load metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The compute_metric function defined in your code calculates ROUGE metrics, \n",
    "# which are important for summarization tasks to check overlap in words and \n",
    "# phrases between the generated summary and the reference summary.\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "    \n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # metric = evaluate.load(\"rouge\")\n",
    "    rouge_results = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    rouge_results = {k: round(v * 100, 4) for k, v in rouge_results.items()}\n",
    "    \n",
    "    results = {\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rouge2\": rouge_results[\"rouge2\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"],\n",
    "        \"rougeLsum\": rouge_results[\"rougeLsum\"],\n",
    "        \"gen_len\": np.mean([np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds])\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "# Define a data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=base_model,  # Optional: some models require it for padding/truncation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the data collator instead of `tokenizer` in the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,  # Replaces tokenizer\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f8a622ea7742a7955c926ddb179977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d560a4e0320c4f3e82b9b6540dbe82ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8015881776809692, 'eval_rouge1': 31.7302, 'eval_rouge2': 9.1392, 'eval_rougeL': 22.8738, 'eval_rougeLsum': 29.1659, 'eval_gen_len': 140.0, 'eval_runtime': 110.2163, 'eval_samples_per_second': 0.181, 'eval_steps_per_second': 0.091, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35310319ed741d5b57b85c0d5d00401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7838362455368042, 'eval_rouge1': 35.4187, 'eval_rouge2': 12.1684, 'eval_rougeL': 27.0919, 'eval_rougeLsum': 31.5229, 'eval_gen_len': 140.0, 'eval_runtime': 106.2856, 'eval_samples_per_second': 0.188, 'eval_steps_per_second': 0.094, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61dd0cb76cb4d798cf3848fbf51bcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7310712337493896, 'eval_rouge1': 32.2426, 'eval_rouge2': 11.5009, 'eval_rougeL': 25.2475, 'eval_rougeLsum': 28.5862, 'eval_gen_len': 140.0, 'eval_runtime': 111.1074, 'eval_samples_per_second': 0.18, 'eval_steps_per_second': 0.09, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858721f37d254742896e84762986a490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7013152837753296, 'eval_rouge1': 34.0449, 'eval_rouge2': 13.143, 'eval_rougeL': 26.1982, 'eval_rougeLsum': 31.2627, 'eval_gen_len': 140.0, 'eval_runtime': 106.1452, 'eval_samples_per_second': 0.188, 'eval_steps_per_second': 0.094, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09878624a244f6ea24b4aacf1fc52ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.686744213104248, 'eval_rouge1': 33.9441, 'eval_rouge2': 12.2572, 'eval_rougeL': 25.3834, 'eval_rougeLsum': 29.9013, 'eval_gen_len': 140.0, 'eval_runtime': 114.6587, 'eval_samples_per_second': 0.174, 'eval_steps_per_second': 0.087, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3fcea8d89f4347b5a8ca73dca7933a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/Users/nanchen/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6724153161048889, 'eval_rouge1': 35.1248, 'eval_rouge2': 12.5808, 'eval_rougeL': 25.9394, 'eval_rougeLsum': 30.2941, 'eval_gen_len': 140.0, 'eval_runtime': 96.1994, 'eval_samples_per_second': 0.208, 'eval_steps_per_second': 0.104, 'epoch': 0.96}\n",
      "{'train_runtime': 794.0306, 'train_samples_per_second': 0.063, 'train_steps_per_second': 0.015, 'train_loss': 0.9207168420155843, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.9207168420155843, metrics={'train_runtime': 794.0306, 'train_samples_per_second': 0.063, 'train_steps_per_second': 0.015, 'total_flos': 52010510450688.0, 'train_loss': 0.9207168420155843, 'epoch': 0.96})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183a0342d0054665a2378a1da954b0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.96\n",
      "{'eval_loss': 0.6724153161048889, 'eval_rouge1': 35.1248, 'eval_rouge2': 12.5808, 'eval_rougeL': 25.9394, 'eval_rougeLsum': 30.2941, 'eval_gen_len': 140.0, 'eval_runtime': 106.4071, 'eval_samples_per_second': 0.188, 'eval_steps_per_second': 0.094, 'epoch': 0.96}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save_pretrained(\"./fine_tuned_bart_summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bart_summarizer/tokenizer_config.json',\n",
       " './fine_tuned_bart_summarizer/special_tokens_map.json',\n",
       " './fine_tuned_bart_summarizer/vocab.json',\n",
       " './fine_tuned_bart_summarizer/merges.txt',\n",
       " './fine_tuned_bart_summarizer/added_tokens.json',\n",
       " './fine_tuned_bart_summarizer/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./fine_tuned_bart_summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in /Users/nanchen/anaconda3/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from huggingface-hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from huggingface-hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from huggingface-hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from huggingface-hub) (6.0)\n",
      "Requirement already satisfied: requests in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from huggingface-hub) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from huggingface-hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nanchen/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8dadce10fa4d76950d743b5636631b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Prompt for login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanchen/anaconda3/lib/python3.11/site-packages/transformers/integrations/peft.py:418: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mia2024/CS5100TextSummarization/commit/9565526e7526334baaa78df17cdd29895e8a8506', commit_message='Upload tokenizer', commit_description='', oid='9565526e7526334baaa78df17cdd29895e8a8506', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mia2024/CS5100TextSummarization', endpoint='https://huggingface.co', repo_type='model', repo_id='Mia2024/CS5100TextSummarization'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./fine_tuned_bart_summarizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_bart_summarizer\")\n",
    "\n",
    "model.push_to_hub(\"Mia2024/CS5100TextSummarization\")\n",
    "tokenizer.push_to_hub(\"Mia2024/CS5100TextSummarization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GenerationConfig, TextStreamer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"Mia2024/CS5100TextSummarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "        min_new_tokens=10,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.9,\n",
    "        top_p=1.0,\n",
    "        top_k=50         \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"facebook/bart-large-cnn\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Summarize the following conversation: \\n###\\n\"\n",
    "suffix = \"\\n### Summary:\"\n",
    "input_text = \"#Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready? #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications. #Person2#: But sir, many employees use Instant Messaging to communicate with their clients. #Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we? #Person2#: This applies to internal and external communications. #Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads. #Person2#: Is that all? #Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\"\n",
    "input_ids = tokenizer.encode(\n",
    "        # input_text,\n",
    "        prefix + input_text + f\"The generated summary should be around {int(0.15 * len(input_text.split()))} words.\" + suffix, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True,\n",
    "        max_length=1024  # Ensure input fits the model's max length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 38182,  3916,  2072,     5,   511,  1607,    35,  1437, 50118,\n",
      "         48134, 50118, 10431, 41761,   134, 10431,    35,  2135,     4, 14820,\n",
      "             6,    38,   240,    47,     7,   185,    10, 28700,  1258,    13,\n",
      "           162,     4,   849, 41761,   176, 10431,    35,  3216,     6, 21958,\n",
      "           734,   849, 41761,   134, 10431,    35,   152,   197,   213,    66,\n",
      "            25,    41, 18592,    12, 23252, 20834,     7,    70,  1321,    30,\n",
      "            42,  1390,     4,  3945,    47,  1227,   116,   849, 41761,   176,\n",
      "         10431,    35,  3216,     6, 21958,     4,  2381,   789,     4,   849,\n",
      "         41761,   134, 10431,    35, 35798,    70,   813,   734, 33355,  1320,\n",
      "             6,    70,   558,  4372,    32,  9393,     7,  1047, 25778,     8,\n",
      "           781, 29966,     4,    20,   304,     9, 26596, 32236,  1767,    30,\n",
      "          1321,   148,   447,   722,    16, 14657,  9986,     4,   849, 41761,\n",
      "           176, 10431,    35,  5348,     6,   473,    42,  3253,     7, 18592,\n",
      "            12, 23252,  4372,   129,   116,  1793,    40,    24,    67, 14058,\n",
      "          6731,  4372,   116,   849, 41761,   134, 10431,    35,    85,   197,\n",
      "          3253,     7,    70,  4372,     6,    45,   129,    11,    42,   558,\n",
      "           227,  1321,     6,    53,    67,   143,   751,  4372,     4,   849,\n",
      "         41761,   176, 10431,    35,   125, 21958,     6,   171,  1321,   304,\n",
      "         26596, 15212,  6257,     7,  8469,    19,    49,  2539,     4,   849,\n",
      "         41761,   134, 10431,    35,   252,    40,    95,    33,     7,   464,\n",
      "            49,  4358,  6448,     4,    38,   218,    75,   236,   143,   111,\n",
      "            65,   634, 26596, 15212,  6257,    11,    42,   558,     4,    85,\n",
      "         39979,   350,   203,    86,   328,   978,     6,  2540,   535,    19,\n",
      "             5,  8168,     4,  4820,    58,    52,   116,   849, 41761,   176,\n",
      "         10431,    35,   152, 11459,     7,  3425,     8,  6731,  4372,     4,\n",
      "           849, 41761,   134, 10431,    35,  3216,     4,  5053,  3200,    54,\n",
      "         31284,    11,   634, 26596, 15212,  6257,    40,    78,  1325,    10,\n",
      "          2892,     8,    28,  2325,    15,  9505,     4,   497,   200,  2970,\n",
      "             6,     5,  3200,    40,   652, 17829,     4,  5053,  1142,  2624,\n",
      "            42,    92,   714,   189,    28,  3660,     7,  1494,  3885,     4,\n",
      "           849, 41761,   176, 10431,    35,  1534,    14,    70,   116,   849,\n",
      "         41761,   134, 10431,    35,  3216,     4,  3401,   120,    42,  8168,\n",
      "         39990,    62,     8,  7664,     7,    70,  1321,   137,   204,  4751,\n",
      "             4,   133,  5129,  4819,   197,    28,   198,  2357,  1617,     4,\n",
      "         50118, 48134, 19584,    35,     2]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_ids, do_sample=True, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  4030,    16,     5,   766,     9,    10,  3924,   341, 17194,\n",
      "             4,    20, 17194,    16,  1887,     7,    28, 16556,    30,     5,\n",
      "           138,     4,    83,  4819,     9,     5, 17194,     4,    20, 11054,\n",
      "            22,  1121,  8304,   113,    64,    28,   341,     7,  6364,   143,\n",
      "           761,     9,  1100,     4,    20, 17194,    18,  5131,  4819,   197,\n",
      "            28,   198,  2357,  1617,     4,     2]])\n"
     ]
    }
   ],
   "source": [
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New is the name of a widely used algorithm. The algorithm is designed to be administered by the company. A summary of the algorithm. The phrase \"Inbox\" can be used to indicate any kind of address. The algorithm's recommended summary should be around 33 words.\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
