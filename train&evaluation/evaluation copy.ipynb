{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanchen/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set Up Logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define ROUGE Evaluation Class\n",
    "class RougeEvaluation:\n",
    "    def __init__(self):\n",
    "        self.rouge_metric = evaluate.load(\"rouge\")\n",
    "        \n",
    "    def compute_rouge_metric(self, generated_summary, reference_summary):\n",
    "        results = self.rouge_metric.compute(\n",
    "            predictions=generated_summary,\n",
    "            references=reference_summary,\n",
    "            use_aggregator=True,\n",
    "            use_stemmer=True,\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Evaluation Function\n",
    "def evaluation_rouge(model, tokenizer, data, generation_config):\n",
    "    # Ensure device compatibility\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Extract dialogue and reference summaries\n",
    "    dialogues = data[\"dialogue\"]\n",
    "    human_summaries = data[\"summary\"]\n",
    "\n",
    "    # Generate summaries\n",
    "    model_summaries = []\n",
    "    prefix = \"Summarize the following dialogue:\\n###\\n\"\n",
    "    suffix = \"\\n### Summary: \"\n",
    "\n",
    "    for idx, dialogue in enumerate(dialogues):\n",
    "        input_text = prefix + dialogue + suffix\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        outputs = model.generate(**inputs, **generation_config)\n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        model_summaries.append(output_text)\n",
    "\n",
    "    # Log progress\n",
    "    logger.info(\"Evaluating summaries...\")\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_evaluator = RougeEvaluation()\n",
    "    results = rouge_evaluator.compute_rouge_metric(model_summaries, human_summaries)\n",
    "\n",
    "    # Calculate average length of generated summaries\n",
    "    generated_lengths = [len(summary.split()) for summary in model_summaries]\n",
    "    average_gen_len = sum(generated_lengths) / len(generated_lengths) if generated_lengths else 0\n",
    "    results[\"gen_len\"] = average_gen_len\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_models(model_names, data, generation_config):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on the same dataset and return a DataFrame with ROUGE scores.\n",
    "    \n",
    "    Parameters:\n",
    "        model_names (list): List of model names to evaluate.\n",
    "        data (Dataset): HuggingFace Dataset object containing 'dialogue' and 'summary'.\n",
    "        generation_config (dict): Configuration for text generation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing ROUGE scores and average generated length for each model.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    for model_name in model_names:\n",
    "        logger.info(f\"Evaluating model: {model_name}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Run evaluation\n",
    "        results = evaluation_rouge(model, tokenizer, data, generation_config)\n",
    "        results[\"model_name\"] = model_name  # Add model name to the results\n",
    "\n",
    "        # Append results to list\n",
    "        results_list.append(results)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    df_results = df_results.set_index(\"model_name\")  # Use model names as index\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to evaluate\n",
    "model_names = [\n",
    "    \"facebook/bart-large-cnn\",\n",
    "    \"google/pegasus-xsum\",\n",
    "    \"Mia2024/CS5100TextSummarization\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "data = Dataset.from_dict({\n",
    "    \"dialogue\": [\n",
    "        \"Hello, how can I help you today? Sure, I can help you book a flight to New York.\",\n",
    "        \"I would like to schedule a doctor's appointment. Is there availability tomorrow morning?\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Customer requested help booking a flight to New York.\",\n",
    "        \"User wants to schedule a doctor's appointment for tomorrow morning.\"\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for text generation\n",
    "generation_config = {\n",
    "    \"max_length\": 50,  # Maximum length of the generated summary\n",
    "    \"num_beams\": 4,    # Use beam search\n",
    "    \"do_sample\": False # Deterministic output\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 11:21:07 - INFO - __main__ - Evaluating model: facebook/bart-large-cnn\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Users/nanchen/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n",
      "12/05/2024 11:21:20 - INFO - __main__ - Evaluating summaries...\n",
      "12/05/2024 11:21:20 - WARNING - evaluate.loading - Using the latest cached version of the module from /Users/nanchen/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Sat Apr 20 12:58:46 2024) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n",
      "12/05/2024 11:21:20 - INFO - absl - Using default tokenizer.\n",
      "12/05/2024 11:21:20 - INFO - __main__ - Evaluating model: google/pegasus-xsum\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "12/05/2024 11:21:30 - INFO - __main__ - Evaluating summaries...\n",
      "12/05/2024 11:21:30 - WARNING - evaluate.loading - Using the latest cached version of the module from /Users/nanchen/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Sat Apr 20 12:58:46 2024) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n",
      "12/05/2024 11:21:30 - INFO - absl - Using default tokenizer.\n",
      "12/05/2024 11:21:30 - INFO - __main__ - Evaluating model: Mia2024/CS5100TextSummarization\n",
      "/Users/nanchen/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "12/05/2024 11:21:38 - INFO - __main__ - Evaluating summaries...\n",
      "12/05/2024 11:21:38 - WARNING - evaluate.loading - Using the latest cached version of the module from /Users/nanchen/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Sat Apr 20 12:58:46 2024) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n",
      "12/05/2024 11:21:38 - INFO - absl - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   rouge1    rouge2    rougeL  rougeLsum  \\\n",
      "model_name                                                                 \n",
      "facebook/bart-large-cnn          0.359091  0.305094  0.359091   0.359091   \n",
      "google/pegasus-xsum              0.109788  0.000000  0.072751   0.072751   \n",
      "Mia2024/CS5100TextSummarization  0.470186  0.368187  0.470186   0.470186   \n",
      "\n",
      "                                 gen_len  \n",
      "model_name                                \n",
      "facebook/bart-large-cnn             34.0  \n",
      "google/pegasus-xsum                 16.5  \n",
      "Mia2024/CS5100TextSummarization     26.5  \n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models and store results in a DataFrame\n",
    "results_df = evaluate_models(model_names, data, generation_config)\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
